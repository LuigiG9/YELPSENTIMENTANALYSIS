{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash ","metadata":{"execution":{"iopub.status.busy":"2022-07-06T08:06:20.511691Z","iopub.execute_input":"2022-07-06T08:06:20.513543Z","iopub.status.idle":"2022-07-06T08:07:04.989567Z","shell.execute_reply.started":"2022-07-06T08:06:20.512083Z","shell.execute_reply":"2022-07-06T08:07:04.988611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install neo4j","metadata":{"execution":{"iopub.status.busy":"2022-07-06T08:07:41.380085Z","iopub.execute_input":"2022-07-06T08:07:41.381407Z","iopub.status.idle":"2022-07-06T08:07:55.885621Z","shell.execute_reply.started":"2022-07-06T08:07:41.381349Z","shell.execute_reply":"2022-07-06T08:07:55.883581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from neo4j import __version__ as neo4j_version\nprint(neo4j_version)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T08:08:09.717212Z","iopub.execute_input":"2022-07-06T08:08:09.717746Z","iopub.status.idle":"2022-07-06T08:08:09.880156Z","shell.execute_reply.started":"2022-07-06T08:08:09.717704Z","shell.execute_reply":"2022-07-06T08:08:09.879240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from neo4j import GraphDatabase\nclass Neo4jConnection:\n    \n    def __init__(self, uri, user, pwd):\n        self.__uri = uri\n        self.__user = user\n        self.__pwd = pwd\n        self.__driver = None\n        try:\n            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n        except Exception as e:\n            print(\"Failed to create the driver:\", e)\n        \n    def close(self):\n        if self.__driver is not None:\n            self.__driver.close()\n        \n    def query(self, query, db=None):\n        assert self.__driver is not None, \"Driver not initialized!\"\n        session = None\n        response = None\n        try: \n            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n            response = list(session.run(query))\n        except Exception as e:\n            print(\"Query failed:\", e)\n        finally: \n            if session is not None:\n                session.close()\n        return response","metadata":{"execution":{"iopub.status.busy":"2022-07-06T08:08:16.429964Z","iopub.execute_input":"2022-07-06T08:08:16.431160Z","iopub.status.idle":"2022-07-06T08:08:16.441052Z","shell.execute_reply.started":"2022-07-06T08:08:16.431112Z","shell.execute_reply":"2022-07-06T08:08:16.440177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conn = Neo4jConnection(uri=\"neo4j+s://1450681e.databases.neo4j.io\", user=\"neo4j\", pwd=\"a9igi8HfgRgXv8VRRr2U91RsTol6cjS2ZO7ZUj6QtPc\")","metadata":{"execution":{"iopub.status.busy":"2022-07-06T08:08:21.947180Z","iopub.execute_input":"2022-07-06T08:08:21.947638Z","iopub.status.idle":"2022-07-06T08:08:21.954012Z","shell.execute_reply.started":"2022-07-06T08:08:21.947603Z","shell.execute_reply":"2022-07-06T08:08:21.952680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from neo4j import GraphDatabase\n\n#establish connection\ngraphdp = GraphDatabase.driver(uri=\"neo4j+s://1450681e.databases.neo4j.io\", auth=(\"neo4j\",\"a9igi8HfgRgXv8VRRr2U91RsTol6cjS2ZO7ZUj6QtPc\"))\n\nsession = graphdp.session()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-06T09:40:16.417989Z","iopub.execute_input":"2022-07-06T09:40:16.418397Z","iopub.status.idle":"2022-07-06T09:40:16.424751Z","shell.execute_reply.started":"2022-07-06T09:40:16.418365Z","shell.execute_reply":"2022-07-06T09:40:16.423398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nquery = ''' MATCH (t:text)-[g:`has written`]-(r:review_id)-[h:`has stars`]-(s:stars) RETURN t,s ''' \nresult = session.run(query).data() \ndf = pd.DataFrame(result)\na2 = spark.createDataFrame(df)\nreview=a2.select('t.text','s.stars')\nreview.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-06T09:42:00.644157Z","iopub.execute_input":"2022-07-06T09:42:00.644590Z","iopub.status.idle":"2022-07-06T09:42:04.880047Z","shell.execute_reply.started":"2022-07-06T09:42:00.644559Z","shell.execute_reply":"2022-07-06T09:42:04.879103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review=a2.select('t.text','s.stars')\nreview.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-06T09:52:53.749616Z","iopub.execute_input":"2022-07-06T09:52:53.751447Z","iopub.status.idle":"2022-07-06T09:52:53.976312Z","shell.execute_reply.started":"2022-07-06T09:52:53.751391Z","shell.execute_reply":"2022-07-06T09:52:53.975334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sparknlp\n\nspark = sparknlp.start()\n\nprint(\"Spark NLP version: {}\".format(sparknlp.version()))\nprint(\"Apache Spark version: {}\".format(spark.version))","metadata":{"execution":{"iopub.status.busy":"2022-07-06T08:17:21.570055Z","iopub.execute_input":"2022-07-06T08:17:21.570779Z","iopub.status.idle":"2022-07-06T08:18:00.754545Z","shell.execute_reply.started":"2022-07-06T08:17:21.570738Z","shell.execute_reply":"2022-07-06T08:18:00.753543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom nltk.stem import WordNetLemmatizer\n\n#!pip install -q findspark\nimport pyspark.sql.functions as F\n#import findspark\nimport os\nimport sys\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib\nfrom mpl_toolkits.mplot3d import Axes3D\nimport math\nimport folium\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom datetime import *\nimport statistics as stats\n# This helps auto print out the items without explixitly using 'print'\nInteractiveShell.ast_node_interactivity = \"all\" \n%matplotlib inline\n# Import PySpark related modules\nimport pyspark\nfrom pyspark.rdd import RDD\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, desc, col, size, array_contains\\\n, isnan, udf, hour, array_min, array_max, countDistinct\nfrom pyspark.sql.types import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-06T09:50:17.980077Z","iopub.execute_input":"2022-07-06T09:50:17.980850Z","iopub.status.idle":"2022-07-06T09:50:19.537772Z","shell.execute_reply.started":"2022-07-06T09:50:17.980813Z","shell.execute_reply":"2022-07-06T09:50:19.537090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Labeling","metadata":{}},{"cell_type":"code","source":"from  pyspark.sql.functions import * \nimport pyspark.sql.functions as F\nfrom pyspark.sql import functions\ndef for_exist_column(df, col, pre):\n    if col in df.columns:\n        return pre(df[col])\n    else:\n        return F.lit(False)\nreview_neutral=review.select('*').where(review['stars']==3)\nreview=review.join(review_neutral,how='leftanti',on='text')\nreview_neutral=review_neutral.withColumn('Target_sentiment',F.when(for_exist_column(review_neutral, 'stars', lambda c: c==3), 'neutral'))\nreview=review.withColumn(\"Target_sentiment\",F.when(for_exist_column(review, 'stars', lambda c: c>3), 'positive').otherwise('negative'))\nreview=review.withColumn( 'text',regexp_replace('text',r',|\\.|&|\\\\|\\||-|_|!|.', ''))\nreview_positive= review.select('text','stars','Target_sentiment').where(review['Target_sentiment'] =='positive')\nreview_positive=review_positive.orderBy(rand()).limit(5000)\nreview_negative= review.select('text','stars','Target_sentiment').where(review['Target_sentiment'] =='negative')\nreview_negative=review_negative.orderBy(rand()).limit(5000)\nreview_neutral=review_neutral.orderBy(rand()).limit(5000)\nreviewf=review_positive.union(review_negative)\nreviewf=reviewf.union(review_neutral)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T09:50:26.306898Z","iopub.execute_input":"2022-07-06T09:50:26.307572Z","iopub.status.idle":"2022-07-06T09:50:26.381585Z","shell.execute_reply.started":"2022-07-06T09:50:26.307520Z","shell.execute_reply":"2022-07-06T09:50:26.380285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La lemmatizzazione Ã¨ il processo di riduzione di una forma flessa di una parola alla sua forma canonica (non marcata), detta lemma.","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport itertools as it\nimport seaborn as sns\nfrom pyspark.ml import Pipeline\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\ndocument= DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\nsentence=SentenceDetector()\\\n.setInputCols(['document'])\\\n.setOutputCol('sentence')\ntoken=Tokenizer()\\\n.setInputCols(['sentence'])\\\n.setOutputCol('token')\nstop_words=StopWordsCleaner.pretrained('stopwords_en','en')\\\n.setInputCols(['token'])\\\n.setOutputCol('cleanTokens')\\\n.setCaseSensitive(False)\nlemmatizer=LemmatizerModel.pretrained()\\\n.setInputCols(['cleanTokens'])\\\n.setOutputCol('lemma')\nfinisher=Finisher()\\\n.setInputCols(['lemma'])\\\n.setOutputCols(['token_features'])\\\n.setOutputAsArray(True)\\\n.setCleanAnnotations(False)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T09:59:02.035241Z","iopub.execute_input":"2022-07-05T09:59:02.035646Z","iopub.status.idle":"2022-07-05T09:59:16.756223Z","shell.execute_reply.started":"2022-07-05T09:59:02.035613Z","shell.execute_reply":"2022-07-05T09:59:16.755457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef plot_histogram(data,x,y,t,u):\n    ## Plot Distribution of the reviews\n    data=data.select(u).toPandas().to_numpy().ravel()\n    \n    plt.style.use('ggplot')\n    fig, ax = plt.subplots(figsize=(7,5))\n    plt.hist(data, bins=50, color = \"tab:red\")\n    ax.set_title(t)\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n\n\n\n\n    \na1=review_neutral.rdd.map(lambda x: [x['text'],len(x['text'])])\nschema = StructType([StructField('text', StringType(), True),StructField('Text_Length', IntegerType(), True)])\na1 = spark.createDataFrame(a1,schema)\na2=review_neutral.rdd.map(lambda x: [x['text'],len(x['text'].split())])\nschema = StructType([StructField('text', StringType(), True),StructField('Word_Count', IntegerType(), True)])\na2 = spark.createDataFrame(a2,schema)\n\na1=a1.join(a2,on='text',how='left')\nnlp_pipeline=Pipeline(\nstages=[document,\nsentence,\ntoken,\nstop_words,\nlemmatizer,\nfinisher])\nmodel=nlp_pipeline.fit(a1)\na=model.transform(a1)\n\nplot_histogram(a, x = \"Text length\", y = \"count of reviews\", t = 'Text length plot of the reviews',u='Text_Length')\nplot_histogram(a, x = \"Word count\", y = \"count of reviews\", t = 'Word Count plot of the reviews',u='Word_Count')\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T14:14:27.33616Z","iopub.execute_input":"2022-07-04T14:14:27.336563Z","iopub.status.idle":"2022-07-04T14:14:43.214307Z","shell.execute_reply.started":"2022-07-04T14:14:27.336531Z","shell.execute_reply":"2022-07-04T14:14:43.213168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h=a.select('lemma.result').toPandas().to_numpy().ravel()\nk=[]\nfor i in range (len(h)):\n    k.append(h[i])\nk=list(map(str, it.chain.from_iterable(k)))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T14:15:28.691629Z","iopub.execute_input":"2022-07-04T14:15:28.692153Z","iopub.status.idle":"2022-07-04T14:15:40.408935Z","shell.execute_reply.started":"2022-07-04T14:15:28.692116Z","shell.execute_reply":"2022-07-04T14:15:40.407822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j=Counter(k)\nj.pop('.')\nj.pop(',')\nj.pop('!')\nj.pop('(')\nj.pop(')')\nj.pop('&')\nj.pop('!!')\nj.pop('-')\nj.pop('=')\nj.pop('\"')\nj.most_common()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T14:15:59.283791Z","iopub.execute_input":"2022-07-04T14:15:59.284185Z","iopub.status.idle":"2022-07-04T14:15:59.410957Z","shell.execute_reply.started":"2022-07-04T14:15:59.284152Z","shell.execute_reply":"2022-07-04T14:15:59.409773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:05:11.157407Z","iopub.execute_input":"2022-07-05T10:05:11.157867Z","iopub.status.idle":"2022-07-05T10:05:11.164084Z","shell.execute_reply.started":"2022-07-05T10:05:11.157829Z","shell.execute_reply":"2022-07-05T10:05:11.162629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set, test_set = reviewf.randomSplit([0.8,0.2],seed=100)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:05:17.930664Z","iopub.execute_input":"2022-07-05T10:05:17.931078Z","iopub.status.idle":"2022-07-05T10:05:17.978524Z","shell.execute_reply.started":"2022-07-05T10:05:17.931046Z","shell.execute_reply":"2022-07-05T10:05:17.977595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint('Train')\ntrain_set.groupBy('Target_sentiment').count().show()\nprint('Test')\ntest_set.groupBy('Target_sentiment').count().show()\n#print(test_set.count())\nprint('General')\nreviewf.groupBy('Target_sentiment').count().show()\n#print(train_set.count())\n","metadata":{"execution":{"iopub.status.busy":"2022-07-01T08:47:32.164621Z","iopub.execute_input":"2022-07-01T08:47:32.165032Z","iopub.status.idle":"2022-07-01T08:56:01.715812Z","shell.execute_reply.started":"2022-07-01T08:47:32.164999Z","shell.execute_reply":"2022-07-01T08:56:01.712676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"document= DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\nsentence=SentenceDetector()\\\n.setInputCols(['document'])\\\n.setOutputCol('sentence')\ntoken=Tokenizer()\\\n.setInputCols(['sentence'])\\\n.setOutputCol('token')\nstop_words=StopWordsCleaner.pretrained('stopwords_en','en')\\\n.setInputCols(['token'])\\\n.setOutputCol('cleanTokens')\\\n.setCaseSensitive(False)\nlemmatizer=LemmatizerModel.pretrained()\\\n.setInputCols(['cleanTokens'])\\\n.setOutputCol('lemma')\nfinisher=Finisher()\\\n.setInputCols(['lemma'])\\\n.setOutputCols(['token_features'])\\\n.setOutputAsArray(True)\\\n.setCleanAnnotations(False)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:05:21.390662Z","iopub.execute_input":"2022-07-05T10:05:21.391179Z","iopub.status.idle":"2022-07-05T10:05:34.47329Z","shell.execute_reply.started":"2022-07-05T10:05:21.391136Z","shell.execute_reply":"2022-07-05T10:05:34.472007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import HashingTF, IDF, StringIndexer, IndexToString\nfrom pyspark.ml.classification import LogisticRegression, NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:05:46.629042Z","iopub.execute_input":"2022-07-05T10:05:46.630154Z","iopub.status.idle":"2022-07-05T10:05:46.635605Z","shell.execute_reply.started":"2022-07-05T10:05:46.630105Z","shell.execute_reply":"2022-07-05T10:05:46.634443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hashTF=HashingTF(inputCol=\"token_features\",outputCol=\"raw_features\")\nidf=IDF(inputCol=\"raw_features\",outputCol=\"features\",minDocFreq=5)\nlabel_strIdx=StringIndexer(inputCol='Target_sentiment',outputCol=\"label\")\nLogReg=LogisticRegression(maxIter=10)\nlabel_idxStr=IndexToString(inputCol=\"label\",outputCol=\"class\")\nnlp_pipeline=Pipeline(\nstages=[document,\n       sentence,\n       token,\n       stop_words,\n       lemmatizer,\n       finisher,\n       hashTF,\n       idf,\n        label_strIdx,\n       LogReg,\n       label_idxStr])\nclas_model=nlp_pipeline.fit(train_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:05:49.161334Z","iopub.execute_input":"2022-07-05T10:05:49.161799Z","iopub.status.idle":"2022-07-05T10:14:21.697737Z","shell.execute_reply.started":"2022-07-05T10:05:49.161766Z","shell.execute_reply":"2022-07-05T10:14:21.695959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=clas_model.transform(test_set)\npred.write.json('pre.json')","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:14:53.102091Z","iopub.execute_input":"2022-07-05T10:14:53.10257Z","iopub.status.idle":"2022-07-05T10:16:29.47009Z","shell.execute_reply.started":"2022-07-05T10:14:53.102517Z","shell.execute_reply":"2022-07-05T10:16:29.469053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator=MulticlassClassificationEvaluator(\nlabelCol='label',predictionCol='prediction',metricName='accuracy')\naccuracy=evaluator.evaluate(pred)\nprint('Accuracy=%g' %(accuracy))\nprint('Test Error =%g' %(1.0-accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:36:29.672272Z","iopub.execute_input":"2022-06-30T09:36:29.672923Z","iopub.status.idle":"2022-06-30T09:42:14.759555Z","shell.execute_reply.started":"2022-06-30T09:36:29.672886Z","shell.execute_reply":"2022-06-30T09:42:14.757343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\ndf_lr=clas_model\\\n.transform(test_set)\\\n.select('Target_sentiment','label','prediction')\\\n.toPandas()\ndf_lr.head()\nprint(classification_report(df_lr.label,df_lr.prediction))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:43:03.693115Z","iopub.execute_input":"2022-06-30T09:43:03.693789Z","iopub.status.idle":"2022-06-30T09:48:42.587535Z","shell.execute_reply.started":"2022-06-30T09:43:03.693753Z","shell.execute_reply":"2022-06-30T09:48:42.586517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pyspark.mllib.evaluation import MulticlassMetrics\npredictionAndLabels = pred.select('label', 'prediction')\nmetrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n\nconfusion_matrix = metrics.confusionMatrix().toArray()\nlabels = [int(l) for l in metrics.call('labels')]\nconfusion_matrix = pd.DataFrame(confusion_matrix , index=labels, columns=labels)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:49:23.520178Z","iopub.execute_input":"2022-06-30T09:49:23.520954Z","iopub.status.idle":"2022-06-30T09:55:07.756674Z","shell.execute_reply.started":"2022-06-30T09:49:23.520918Z","shell.execute_reply":"2022-06-30T09:55:07.754191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-06-30T10:29:19.226486Z","iopub.execute_input":"2022-06-30T10:29:19.227048Z","iopub.status.idle":"2022-06-30T10:29:19.39203Z","shell.execute_reply.started":"2022-06-30T10:29:19.227005Z","shell.execute_reply":"2022-06-30T10:29:19.391215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 approccio: Naive Bayes","metadata":{}},{"cell_type":"code","source":"hashTF=HashingTF(inputCol=\"token_features\",outputCol=\"raw_features\")\nidf=IDF(inputCol=\"raw_features\",outputCol=\"features\",minDocFreq=5)\nlabel_strIdx=StringIndexer(inputCol='Target_sentiment',outputCol=\"label\")\nbayes_class=NaiveBayes(smoothing=111)\nlabel_idxStr=IndexToString(inputCol=\"label\",outputCol=\"class\")\nnlp_pipeline_b=Pipeline(\nstages=[document,\n       sentence,\n       token,\n       stop_words,\n       lemmatizer,\n       finisher,\n       hashTF,\n       idf,\n        label_strIdx,\n       bayes_class,\n       label_idxStr])\nclas_model_naive=nlp_pipeline_b.fit(train_set)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:48:41.375557Z","iopub.execute_input":"2022-06-30T15:48:41.376368Z","iopub.status.idle":"2022-06-30T16:01:15.946472Z","shell.execute_reply.started":"2022-06-30T15:48:41.376331Z","shell.execute_reply":"2022-06-30T16:01:15.943481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_2=clas_model_naive.transform(test_set)\nevaluator=MulticlassClassificationEvaluator(\nlabelCol='label',predictionCol='prediction',metricName='accuracy')\naccuracy_2=evaluator.evaluate(pred_2)\nprint('Accuracy=%g' %(accuracy_2))\nprint('Test Error =%g' %(1.0-accuracy_2))\nfrom sklearn.metrics import classification_report,accuracy_score\ndf_lr_2=clas_model_naive\\\n.transform(test_set)\\\n.select('Target_sentiment','label','prediction')\\\n.toPandas()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T16:02:00.427776Z","iopub.execute_input":"2022-06-30T16:02:00.42825Z","iopub.status.idle":"2022-06-30T16:08:22.09414Z","shell.execute_reply.started":"2022-06-30T16:02:00.428214Z","shell.execute_reply":"2022-06-30T16:08:22.091792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_lr_2.head()\nprint(classification_report(df_lr_2.label,df_lr_2.prediction))\nimport pandas as pd\nfrom pyspark.mllib.evaluation import MulticlassMetrics\npredictionAndLabels_2 = pred_2.select('label', 'prediction')\nmetrics_2 = MulticlassMetrics(predictionAndLabels_2.rdd.map(lambda x: tuple(map(float, x))))\n\nconfusion_matrix2 = metrics_2.confusionMatrix().toArray()\nlabels_2 = [int(l) for l in metrics_2.call('labels')]\nconfusion_matrix2 = pd.DataFrame(confusion_matrix2 , index=labels_2, columns=labels_2)\nconfusion_matrix2","metadata":{"execution":{"iopub.status.busy":"2022-06-30T16:10:10.748616Z","iopub.execute_input":"2022-06-30T16:10:10.749133Z","iopub.status.idle":"2022-06-30T16:13:16.658167Z","shell.execute_reply.started":"2022-06-30T16:10:10.74909Z","shell.execute_reply":"2022-06-30T16:13:16.657103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"UNIVERSAL SENTENCE DETECTOR","metadata":{}},{"cell_type":"code","source":"from sparknlp.pretrained import PretrainedPipeline","metadata":{"execution":{"iopub.status.busy":"2022-07-01T14:02:35.622065Z","iopub.execute_input":"2022-07-01T14:02:35.622683Z","iopub.status.idle":"2022-07-01T14:02:35.628779Z","shell.execute_reply.started":"2022-07-01T14:02:35.622641Z","shell.execute_reply":"2022-07-01T14:02:35.627621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndocument = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n    \n\nuse = UniversalSentenceEncoder.pretrained()\\\n .setInputCols([\"document\"])\\\n .setOutputCol(\"sentence_embeddings\")\n\nclasssifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"sentence_embeddings\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"Target_sentiment\")\\\n  .setMaxEpochs(10)\\\n  .setEnableOutputLogs(True)\nuse_clf_pipeline = Pipeline(\n    stages = [\n        document,\n        use,\n        classsifierdl\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-07-01T14:56:59.415261Z","iopub.execute_input":"2022-07-01T14:56:59.415741Z","iopub.status.idle":"2022-07-01T14:57:03.018416Z","shell.execute_reply.started":"2022-07-01T14:56:59.4157Z","shell.execute_reply":"2022-07-01T14:57:03.017185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_pipelineModel = use_clf_pipeline.fit(train_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T14:57:07.473193Z","iopub.execute_input":"2022-07-01T14:57:07.473951Z","iopub.status.idle":"2022-07-01T15:09:01.697236Z","shell.execute_reply.started":"2022-07-01T14:57:07.473908Z","shell.execute_reply":"2022-07-01T15:09:01.694633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=use_pipelineModel.transform(test_set)\nfrom sklearn.metrics import classification_report, accuracy_score\ndf=preds.select('text','Target_sentiment','class.result').toPandas()\ndf['result']=df['result'].apply(lambda x: x[0])\nprint(classification_report(df.Target_sentiment,df.result))\nprint(accuracy_score(df.Target_sentiment,df.result))","metadata":{"execution":{"iopub.status.busy":"2022-07-01T15:09:19.509136Z","iopub.execute_input":"2022-07-01T15:09:19.509707Z","iopub.status.idle":"2022-07-01T15:13:02.749587Z","shell.execute_reply.started":"2022-07-01T15:09:19.50966Z","shell.execute_reply":"2022-07-01T15:13:02.748325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"BERT","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport itertools as it\nimport seaborn as sns\nfrom pyspark.ml import Pipeline\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\ndocument = DocumentAssembler()\\\n      .setInputCol(\"text\")\\\n      .setOutputCol(\"document\")\n\nbert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n      .setInputCols([\"document\"])\\\n      .setOutputCol(\"sentence_embeddings\")\n\n\nclasssifierdl = ClassifierDLApproach()\\\n      .setInputCols([\"sentence_embeddings\"])\\\n      .setOutputCol(\"class\")\\\n      .setLabelColumn(\"Target_sentiment\")\\\n      .setMaxEpochs(10)\\\n      .setEnableOutputLogs(True)\\\n      .setLr(0.001)\n\nbert_sent_clf_pipeline = Pipeline(\n    stages = [\n        document,\n        bert_sent,\n        classsifierdl\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-07-04T16:19:47.374029Z","iopub.execute_input":"2022-07-04T16:19:47.374468Z","iopub.status.idle":"2022-07-04T16:20:17.129137Z","shell.execute_reply.started":"2022-07-04T16:19:47.374433Z","shell.execute_reply":"2022-07-04T16:20:17.127847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_sent_pipelineModel = bert_sent_clf_pipeline.fit(train_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T16:21:53.34618Z","iopub.execute_input":"2022-07-04T16:21:53.346598Z","iopub.status.idle":"2022-07-04T16:56:02.911082Z","shell.execute_reply.started":"2022-07-04T16:21:53.346563Z","shell.execute_reply":"2022-07-04T16:56:02.905715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npreds = bert_sent_pipelineModel.transform(test_set)\n\npreds_df = preds.select('Target_sentiment','text',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['Target_sentiment']))","metadata":{"execution":{"iopub.status.busy":"2022-07-04T16:58:26.332449Z","iopub.execute_input":"2022-07-04T16:58:26.33379Z","iopub.status.idle":"2022-07-04T17:03:40.297486Z","shell.execute_reply.started":"2022-07-04T16:58:26.333742Z","shell.execute_reply":"2022-07-04T17:03:40.296364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.mllib.evaluation import MulticlassMetrics\npredictionAndLabels_2 = preds.select('label', 'prediction')\nmetrics_2 = MulticlassMetrics(predictionAndLabels_2.rdd.map(lambda x: tuple(map(float, x))))\n\nconfusion_matrix2 = metrics_2.confusionMatrix().toArray()\nlabels_2 = [int(l) for l in metrics_2.call('labels')]\nconfusion_matrix2 = pd.DataFrame(confusion_matrix2 , index=labels_2, columns=labels_2)\nconfusion_matrix2","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:04:59.464646Z","iopub.execute_input":"2022-07-04T17:04:59.465148Z","iopub.status.idle":"2022-07-04T17:04:59.860824Z","shell.execute_reply.started":"2022-07-04T17:04:59.465113Z","shell.execute_reply":"2022-07-04T17:04:59.858452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GRAFICI","metadata":{}},{"cell_type":"code","source":"h=[0.88,0.69,0.78,0.76,0.93]\nlab=['LogReg','Naive','RandomF','U.S.E','BERT']\nimport matplotlib.pyplot as plt\n\nplt.bar(lab,h)\nplt.xlabel('Approach')\nplt.ylabel('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-07-05T09:10:26.947176Z","iopub.execute_input":"2022-07-05T09:10:26.947601Z","iopub.status.idle":"2022-07-05T09:10:27.132117Z","shell.execute_reply.started":"2022-07-05T09:10:26.947564Z","shell.execute_reply":"2022-07-05T09:10:27.13142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"laab=['negative','neutral','positive']\nerrorilog=[129,111,105]\nerrNaiv=[675,2,249]\nerrRand=[6,491,165]\nerrBert=[110,19,61]\nerr=[(129+675+6+110)/4,(111+2+491+19)/4,(105+249+165+61)/4]\nplt.bar(laab,err)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T09:22:05.64877Z","iopub.execute_input":"2022-07-05T09:22:05.649814Z","iopub.status.idle":"2022-07-05T09:22:05.812116Z","shell.execute_reply.started":"2022-07-05T09:22:05.64977Z","shell.execute_reply":"2022-07-05T09:22:05.811098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"Random Forest","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nhashTF=HashingTF(inputCol=\"token_features\",outputCol=\"raw_features\")\nidf=IDF(inputCol=\"raw_features\",outputCol=\"features\",minDocFreq=5)\nlabel_strIdx=StringIndexer(inputCol='Target_sentiment',outputCol=\"label\")\nlabel_idxStr=IndexToString(inputCol=\"label\",outputCol=\"class\")\nnlp_pipeline_b=Pipeline(\nstages=[document,\n       sentence,\n       token,\n       stop_words,\n       lemmatizer,\n       finisher,\n       hashTF,\n       idf,\n        label_strIdx,\n       label_idxStr])\nclas_modell=nlp_pipeline_b.fit(train_set)\nutile_test=clas_modell.transform(test_set)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-01T10:45:21.317641Z","iopub.execute_input":"2022-07-01T10:45:21.318222Z","iopub.status.idle":"2022-07-01T10:52:14.616765Z","shell.execute_reply.started":"2022-07-01T10:45:21.318188Z","shell.execute_reply":"2022-07-01T10:52:14.612368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utile_train=clas_modell.transform(train_set)\nalgo = RandomForestClassifier(featuresCol='features', labelCol='label')\n\nmodel = algo.fit(utile_train)\npredictions = model.transform(utile_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T10:54:10.781134Z","iopub.execute_input":"2022-07-01T10:54:10.781627Z","iopub.status.idle":"2022-07-01T11:02:57.143589Z","shell.execute_reply.started":"2022-07-01T10:54:10.781592Z","shell.execute_reply":"2022-07-01T11:02:57.141489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator=MulticlassClassificationEvaluator(\nlabelCol='label',predictionCol='prediction',metricName='accuracy')\naccuracy_2=evaluator.evaluate(predictions)\nprint('Accuracy=%g' %(accuracy_2))\nprint('Test Error =%g' %(1.0-accuracy_2))\nfrom sklearn.metrics import classification_report,accuracy_score\ndf_lr_2=clas_modell\\\n.transform(test_set)\\\n.select('Target_sentiment','label','prediction')\\\n.toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T11:04:00.850952Z","iopub.execute_input":"2022-07-01T11:04:00.85145Z","iopub.status.idle":"2022-07-01T11:07:40.888342Z","shell.execute_reply.started":"2022-07-01T11:04:00.851413Z","shell.execute_reply":"2022-07-01T11:07:40.883027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\ndf_lr=model\\\n.transform(utile_test)\\\n.select('Target_sentiment','label','prediction')\\\n.toPandas()\ndf_lr.head()\nprint(classification_report(df_lr.label,df_lr.prediction))\nimport pandas as pd\nfrom pyspark.mllib.evaluation import MulticlassMetrics\npredictionAndLabels = predictions.select('label', 'prediction')\nmetrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n\nconfusion_matrix = metrics.confusionMatrix().toArray()\nlabels = [int(l) for l in metrics.call('labels')]\nconfusion_matrix = pd.DataFrame(confusion_matrix , index=labels, columns=labels)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T11:13:06.258573Z","iopub.execute_input":"2022-07-01T11:13:06.260039Z","iopub.status.idle":"2022-07-01T11:20:10.816734Z","shell.execute_reply.started":"2022-07-01T11:13:06.259989Z","shell.execute_reply":"2022-07-01T11:20:10.814816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-07-01T11:21:34.932482Z","iopub.execute_input":"2022-07-01T11:21:34.932935Z","iopub.status.idle":"2022-07-01T11:21:34.969404Z","shell.execute_reply.started":"2022-07-01T11:21:34.9329Z","shell.execute_reply":"2022-07-01T11:21:34.968543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SALVATAGGIO SU MONGO","metadata":{}},{"cell_type":"code","source":"pip install pymongo","metadata":{"execution":{"iopub.status.busy":"2022-07-05T09:33:03.730276Z","iopub.execute_input":"2022-07-05T09:33:03.730701Z","iopub.status.idle":"2022-07-05T09:33:17.180677Z","shell.execute_reply.started":"2022-07-05T09:33:03.730668Z","shell.execute_reply":"2022-07-05T09:33:17.179444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mongodb+srv://luigigarofalo:<password>@cluster0.sy8pl.mongodb.net/?retryWrites=true&w=majority\n# Imports\n#!pip install dnspython\n#!pip install pymongo\nimport pandas as pd\nfrom pymongo import MongoClient\n# Load csv dataset\ndata =pred.select('text','label','prediction').toPandas()\n# Connect to MongoDB\nclient =  MongoClient(\"mongodb+srv://luigigarofalo:2SenxCosx<@cluster0.sy8pl.mongodb.net/?retryWrites=true&w=majority\")\ndb = client['results']\ncollection = db['logreg']\ndata.reset_index(inplace=True)\ndata_dict = data.to_dict(\"records\")\n# Insert collection\ncollection.insert_many(data_dict)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T10:35:53.223486Z","iopub.execute_input":"2022-07-05T10:35:53.223933Z","iopub.status.idle":"2022-07-05T10:37:36.530548Z","shell.execute_reply.started":"2022-07-05T10:35:53.2239Z","shell.execute_reply":"2022-07-05T10:37:36.529438Z"},"trusted":true},"execution_count":null,"outputs":[]}]}